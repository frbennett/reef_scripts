{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "elementMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\CSIRO_GBR_ElementMapper.csv'\n",
    "#Names from element mapper\n",
    "regID = 'RegID'\n",
    "riverCol = 'Major River'\n",
    "elementCol = 'Network Element'\n",
    "\n",
    "simMapperCSV = r'P:\\projects\\eReefs\\GBRF_SpeedScenarios\\GBRF_ScenarioExtension_ResultsMapperFiltered.csv'\n",
    "#Names from sim mapper, use RegID from above\n",
    "basinFolderName = 'BasinFolder'\n",
    "newSimFolderName = 'SIMCODE'\n",
    "#Switch these for TS Dependent or EMCMonthly\n",
    "existSimFolderName = 'TSDependentSubDir'\n",
    "#existSimFolderName = 'EMCMonthly2021SubDir'\n",
    "\n",
    "#Switch these for parent location of TS Dependent or EMCMonthly results\n",
    "#TSDependent\n",
    "parentResultsDir = r'E:\\GBRFScenarios\\Results'\n",
    "addRegIdDIR = True\n",
    "outPath = r'E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet'\n",
    "#Empirical\n",
    "#parentResultsDir = r'P:\\projects\\RC10_ResultsSets\\Models\\results'\n",
    "#addRegIdDIR = False\n",
    "#outPath = r'E:\\GBRFScenarios\\CSIROAggregatedOutlets\\EmpiricalEMC'\n",
    "\n",
    "#, 'Sediment - Coarse'\n",
    "\n",
    "Constituents = ['Sediment - Fine', 'N_Particulate', 'N_DIN', 'N_DON', 'P_Particulate', 'P_DOP', 'P_FRP']\n",
    "#Constituents = ['Ametryn', 'S-metolachlor', 'Atrazine', 'Diuron', 'Hexazinone', 'Tebuthiuron', '24-D'\n",
    "#                , 'Paraquat', 'Glyphosate', 'Chlorsulfuron', 'Diquat', 'Fluroxypyr', 'Haloxyfop', 'Imazapic'\n",
    "#               , 'Imazethapyr', 'Isoxaflutole', 'MCPA', 'Metribuzin', 'Metsulfuron-methyl', 'Pendimethalin', 'Simazine'\n",
    "#               , 'Terbuthylazin', 'Trifluralin', 'Aciflurofen', 'Chlorpyrifos', 'Fipronil', 'Imidacloprid', 'Prometryn', 'Triclopyr']\n",
    "\n",
    "#regionIDs = ['BU','FI','CY','BM', 'WT', 'MW']\n",
    "#,'CY':'CY Rebuild 2015'\n",
    "#'BU':'Burdekin Rebuild 2014',\n",
    "regionIDs = {'BM':'Burnett Mary Rebuild 2014','FI':'FI RC2019',\n",
    "              'MW':'MW_RC10', 'WT':'WT_RC10'}\n",
    "#elementMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\BurdekinElementMapper.csv'\n",
    "#mainPath = r'P:\\projects\\MichaelWarne\\Models\\results\\Burdekin Rebuild 2014\\BU_msPAF_Calibrated_503\\TimeSeries'\n",
    "#outPath = r'C:\\DDrive\\RC10PesticideProjs\\CSIROAggregatedOutlets\\Burdekin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outPath):\n",
    "    os.makedirs(outPath)\n",
    "    print(\"Made dir: \" + outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##This bit now needs to happen inside of loop\n",
    "# RiverElements = {}\n",
    "# elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "# for index, row in elementsTable.iterrows():\n",
    "#     if not row[riverCol] in RiverElements.keys():\n",
    "#         RiverElements[row[riverCol]] = []\n",
    "    \n",
    "#     RiverElements[row[riverCol]].append(row[elementCol])\n",
    "\n",
    "\n",
    "# #print(RiverElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allElementsTable = pd.read_csv(elementMapperCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDetailsDF = pd.read_csv(simMapperCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsPathInfo(regionIDString, scenarioString):\n",
    "    tsPath = ''\n",
    "    if addRegIdDIR:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDString + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    else:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    return tsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doThisSim(thisRegID, thisBasinFolderName, thisNewSimFolderName, thisExistSimFolderName, theseRiverElements):\n",
    "    \n",
    "    #Going to need to set up folders\n",
    "    #basinOutputStr = outPath + '\\\\' + thisBasinFolderName\n",
    "    basinOutputStr = os.path.join(outPath, thisBasinFolderName)\n",
    "    if not os.path.exists(basinOutputStr):\n",
    "        os.makedirs(basinOutputStr)\n",
    "        print(\"Made dir: \" + basinOutputStr)\n",
    "    \n",
    "    \n",
    "    simOutputStr = os.path.join(basinOutputStr, thisNewSimFolderName)\n",
    "    if not os.path.exists(simOutputStr):\n",
    "        os.makedirs(simOutputStr)\n",
    "        print(\"Made dir: \" + simOutputStr)    \n",
    "    \n",
    "    #Set up the totals first\n",
    "    RiverElements = {}\n",
    "    # elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "    for index, row in theseRiverElements.iterrows():\n",
    "         if not row[riverCol] in RiverElements.keys():\n",
    "            RiverElements[row[riverCol]] = []\n",
    "    \n",
    "         RiverElements[row[riverCol]].append(row[elementCol])\n",
    "    \n",
    "    riverTotals = {}\n",
    "    for theRiver in RiverElements.keys():\n",
    "        riverTotals[theRiver] = pd.DataFrame()\n",
    "\n",
    "    #Now data\n",
    "    mainPath = getResultsPathInfo(thisRegID, thisExistSimFolderName)\n",
    "\n",
    "    theFlowsDIR = os.path.join(mainPath,'Flows')\n",
    "    for theRiver in RiverElements.keys():\n",
    "        #We'll want flows first\n",
    "        eleCount = 0\n",
    "        flowFrame = pd.DataFrame()\n",
    "        outputFields = ['Date']\n",
    "        for theElement in RiverElements[theRiver]:\n",
    "            for fname in os.listdir(theFlowsDIR):\n",
    "                if theElement in fname:\n",
    "                    #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                    flowFilePath = os.path.join(theFlowsDIR,fname)\n",
    "                    flowData = pd.read_csv(flowFilePath)\n",
    "                    flowData.columns = ['Date', theElement]\n",
    "                    #riverTotals[theRiver].columns.drop(theElement)\n",
    "                    outputFields.append(theElement)\n",
    "                    if eleCount == 0:\n",
    "                        #Get the structure for our totals first\n",
    "                        riverTotals[theRiver] = flowData\n",
    "                        riverTotals[theRiver].drop(theElement, axis=1, inplace=True)\n",
    "                        #Now process stuff\n",
    "                        flowFrame = flowData\n",
    "                        #flowFrame.columns = ['Date', theElement]\n",
    "                        #flowFrame['Date'] = flowData['Date']\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values#This sould give the 2nd column\n",
    "                    else:\n",
    "                        #Join...??\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values\n",
    "                        flowFrame = pd.merge(flowFrame, flowData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                    eleCount += 1\n",
    "        #Totals\n",
    "        #The .iloc[:, 1:] should exclude our first col, Date, from the sum\n",
    "        #flowFrame['Flow_cumecs'] = flowFrame.iloc[:, 1:].sum(axis=1)\n",
    "        flowFrame['Flow_cumecs'] = flowFrame.sum(axis=1)\n",
    "        #Write out flow\n",
    "        theOutFlowFile = os.path.join(outPath, \"Flow \" + theRiver + \" cubicmetrespersecond.csv\")\n",
    "        flowFrame.to_csv(theOutFlowFile, index=False)\n",
    "        #Need to join flow Total to RiverTotals\n",
    "        riverTotals[theRiver] = pd.merge(riverTotals[theRiver], flowFrame[['Date', 'Flow_cumecs']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "    #Now water quality\n",
    "    for theWQBit in Constituents:\n",
    "        wqPath = os.path.join(mainPath, theWQBit)\n",
    "        if not os.path.exists(wqPath):\n",
    "            continue\n",
    "    \n",
    "        for theRiver in RiverElements.keys():\n",
    "            #We'll want flows first\n",
    "            eleCount = 0\n",
    "            wqFrame = pd.DataFrame()\n",
    "            outputFields = ['Date']\n",
    "            for theElement in RiverElements[theRiver]:\n",
    "                for fname in os.listdir(wqPath):\n",
    "                    if theElement in fname:\n",
    "                        #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                        wqFilePath = os.path.join(wqPath,fname)\n",
    "                        wqData = pd.read_csv(wqFilePath)\n",
    "                        wqData.columns = ['Date', theElement]\n",
    "                        outputFields.append(theElement)\n",
    "                        if eleCount == 0:\n",
    "                            wqFrame = wqData\n",
    "                        else:\n",
    "                            wqFrame = pd.merge(wqFrame, wqData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                        eleCount += 1\n",
    "            #Totals\n",
    "            #wqFrame[theRiver + '_kg'] = wqFrame.iloc[:, 1:].sum(axis=1)\n",
    "            wqFrame[theRiver + '_kg'] = wqFrame.sum(axis=1)\n",
    "            #Write out flow\n",
    "            theOutFlowFile = os.path.join(outPath, theWQBit + \" \" + theRiver + \" kilograms.csv\")\n",
    "            wqFrame.to_csv(theOutFlowFile, index=False)\n",
    "            #Join to totals\n",
    "            #rename constituent total first\n",
    "            #print('theRiver: ' + theRiver)\n",
    "            #print('theWQBit: ' + theWQBit)\n",
    "            wqFrame.rename(columns={theRiver + '_kg':theWQBit + '_kg'}, inplace=True)\n",
    "            riverTotals[theRiver] = pd.merge(riverTotals[theRiver], wqFrame[['Date', theWQBit + '_kg']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "\n",
    "    #Now write totals files\n",
    "    for theRiver in RiverElements.keys():\n",
    "        theOutTotalsFile = os.path.join(outPath, theRiver + \" Totals.csv\")\n",
    "        riverTotals[theRiver].to_csv(theOutTotalsFile, index=False)\n",
    "\n",
    "\n",
    "    print(\"Done \" + thisNewSimFolderName + \" at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing this simulation: FITZUSLENOPASS at 2021-09-28 12:06:26.412180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done FITZUSLENOPASS at 2021-09-28 12:06:34.148923\n",
      "Doing this simulation: FITZUSLEWPASS at 2021-09-28 12:06:34.148923\n",
      "Done FITZUSLEWPASS at 2021-09-28 12:06:41.680676\n",
      "Doing this simulation: FITZBANK at 2021-09-28 12:06:41.680676\n",
      "Done FITZBANK at 2021-09-28 12:06:49.124784\n",
      "Doing this simulation: MACKUSLENOPASS at 2021-09-28 12:06:49.140586\n",
      "Done MACKUSLENOPASS at 2021-09-28 12:06:56.680416\n",
      "Doing this simulation: MACKUSLEWPASS at 2021-09-28 12:06:56.696048\n",
      "Done MACKUSLEWPASS at 2021-09-28 12:07:06.938860\n",
      "Doing this simulation: HERBUSLENOPASS at 2021-09-28 12:07:06.938860\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\HERBUSLENOPASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node4_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node4_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node1_x'} in the result is deprecated and will raise a MergeError in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done HERBUSLENOPASS at 2021-09-28 12:07:28.490454\n",
      "Doing this simulation: HERBUSLEWPASS at 2021-09-28 12:07:28.490454\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\HERBUSLEWPASS\n",
      "Done HERBUSLEWPASS at 2021-09-28 12:07:51.164735\n",
      "Doing this simulation: HERBBANK at 2021-09-28 12:07:51.164735\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\HERBBANK\n",
      "Done HERBBANK at 2021-09-28 12:08:12.929433\n",
      "Doing this simulation: MARYBANK at 2021-09-28 12:08:12.945058\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\BurnettMary\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\BurnettMary\\MARYBANK\n",
      "Done MARYBANK at 2021-09-28 12:08:23.821825\n",
      "Doing this simulation: WETFERTDB at 2021-09-28 12:08:23.821825\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WETFERTDB\n",
      "Done WETFERTDB at 2021-09-28 12:08:45.709236\n",
      "Doing this simulation: WETMUDDB at 2021-09-28 12:08:45.710242\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WETMUDDB\n",
      "Done WETMUDDB at 2021-09-28 12:09:08.123780\n",
      "Doing this simulation: WETFERTCB at 2021-09-28 12:09:08.123780\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WETFERTCB\n",
      "Done WETFERTCB at 2021-09-28 12:09:29.927743\n",
      "Doing this simulation: WETMUDCB at 2021-09-28 12:09:29.943363\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WETMUDCB\n",
      "Done WETMUDCB at 2021-09-28 12:09:51.238515\n",
      "Doing this simulation: WETCANERET at 2021-09-28 12:09:51.238515\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WETCANERET\n",
      "Done WETCANERET at 2021-09-28 12:10:14.402057\n",
      "Doing this simulation: PLANFERTDB at 2021-09-28 12:10:14.402057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node5_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Outlet Node5_x'} in the result is deprecated and will raise a MergeError in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done PLANFERTDB at 2021-09-28 12:10:26.851999\n",
      "Doing this simulation: PLANMUDDB at 2021-09-28 12:10:26.867642\n",
      "Done PLANMUDDB at 2021-09-28 12:10:38.326104\n",
      "Doing this simulation: PLANFERTCB at 2021-09-28 12:10:38.341745\n",
      "Done PLANFERTCB at 2021-09-28 12:10:49.397065\n",
      "Doing this simulation: PLANMUDCB at 2021-09-28 12:10:49.412689\n",
      "Done PLANMUDCB at 2021-09-28 12:11:00.696774\n",
      "Doing this simulation: FIBASE at 2021-09-28 12:11:00.712397\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\Fitzroy\\FIBASE\n",
      "Done FIBASE at 2021-09-28 12:11:12.151468\n",
      "Doing this simulation: FIPREDEV at 2021-09-28 12:11:12.158174\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\Fitzroy\\FIPREDEV\n",
      "Done FIPREDEV at 2021-09-28 12:11:20.501922\n",
      "Doing this simulation: WTBASE at 2021-09-28 12:11:20.501922\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WTBASE\n",
      "Done WTBASE at 2021-09-28 12:11:42.342422\n",
      "Doing this simulation: WTPREDEV at 2021-09-28 12:11:42.344449\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\WetTropics\\WTPREDEV\n",
      "Done WTPREDEV at 2021-09-28 12:12:00.206202\n",
      "Doing this simulation: BMBASE at 2021-09-28 12:12:00.226874\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\BurnettMary\\BMBASE\n",
      "Done BMBASE at 2021-09-28 12:12:09.107825\n",
      "Doing this simulation: BMPREDEV at 2021-09-28 12:12:09.123267\n",
      "Made dir: E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet\\BurnettMary\\BMPREDEV\n",
      "Done BMPREDEV at 2021-09-28 12:12:18.493375\n",
      "Doing this simulation: MWBASE at 2021-09-28 12:12:18.493375\n",
      "Done MWBASE at 2021-09-28 12:12:29.841142\n",
      "Doing this simulation: MWPREDEV at 2021-09-28 12:12:29.863201\n",
      "Done MWPREDEV at 2021-09-28 12:12:41.559520\n",
      "All finished at 2021-09-28 12:12:41.559520\n"
     ]
    }
   ],
   "source": [
    "#This is where we will loop through sims\n",
    "for index, row in simDetailsDF.iterrows():\n",
    "    if row[regID] in regionIDs.keys():\n",
    "        print(\"Doing this simulation: \" + row[newSimFolderName] + \" at \" + str(datetime.datetime.now()))\n",
    "        #Filter out the river elements for this region\n",
    "        filteredElements = allElementsTable.loc[allElementsTable[regID] == row[regID]]\n",
    "        doThisSim(row[regID], row[basinFolderName], row[newSimFolderName], row[existSimFolderName], filteredElements)\n",
    "\n",
    "print(\"All finished at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
