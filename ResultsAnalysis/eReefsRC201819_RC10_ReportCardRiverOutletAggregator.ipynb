{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "elementMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\CSIRO_GBR_ElementMapperBundledOutlets.csv'\n",
    "#Names from element mapper\n",
    "regID = 'RegID'\n",
    "riverCol = 'Major River'\n",
    "elementCol = 'Network Element'\n",
    "\n",
    "#Use this to write out (or not) individual outlet files\n",
    "WriteIndividual = False\n",
    "\n",
    "#GBRF Scenarios\n",
    "#simMapperCSV = r'P:\\projects\\eReefs\\GBRF_SpeedScenarios\\GBRF_ScenarioExtension_ResultsMapperFiltered.csv'\n",
    "\n",
    "##All sediment strategies together\n",
    "#simMapperCSV = r'P:\\projects\\eReefs\\GBRF_SpeedScenarios\\GBRF_ScenarioExtension_ResultsMapperFiltered_FSED_ALL.csv'\n",
    "\n",
    "#Standard eReefs baseline/pre-dev\n",
    "simMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\CSIRO_eReefsStandard_ResultsMapper_Nov2021.csv'\n",
    "\n",
    "\n",
    "#Names from sim mapper, use RegID from above\n",
    "basinFolderName = 'BasinFolder'\n",
    "newSimFolderName = 'SIMCODE'\n",
    "#Switch these for TS Dependent or EMCMonthly\n",
    "existSimFolderName = 'TSDependentSubDir'\n",
    "#existSimFolderName = 'EMCMonthly2021SubDir'\n",
    "\n",
    "#Switch these for parent location of TS Dependent or EMCMonthly results\n",
    "#TSDependent\n",
    "# parentResultsDir = r'E:\\GBRFScenarios\\Results'\n",
    "# addRegIdDIR = True\n",
    "# outPath = r'E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet'\n",
    "#Empirical\n",
    "parentResultsDir = r'P:\\projects\\RC10_ResultsSets\\Models\\results'\n",
    "addRegIdDIR = False\n",
    "#outPath = r'E:\\GBRFScenarios\\CSIROAggregatedOutlets\\EmpiricalEMC'\n",
    "##outPath = r'P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\EmpiricalEMCNov2021'\n",
    "#outPath = r'P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\EmpiricalEMCNov2021WithIndividual'\n",
    "outPath = r'P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018'\n",
    "#outPath = r'P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018WithIndividual'\n",
    "\n",
    "#, 'Sediment - Coarse'\n",
    "\n",
    "Constituents = ['Sediment - Fine', 'N_Particulate', 'N_DIN', 'N_DON', 'P_Particulate', 'P_DOP', 'P_FRP']\n",
    "#Constituents = ['Ametryn', 'S-metolachlor', 'Atrazine', 'Diuron', 'Hexazinone', 'Tebuthiuron', '24-D'\n",
    "#                , 'Paraquat', 'Glyphosate', 'Chlorsulfuron', 'Diquat', 'Fluroxypyr', 'Haloxyfop', 'Imazapic'\n",
    "#               , 'Imazethapyr', 'Isoxaflutole', 'MCPA', 'Metribuzin', 'Metsulfuron-methyl', 'Pendimethalin', 'Simazine'\n",
    "#               , 'Terbuthylazin', 'Trifluralin', 'Aciflurofen', 'Chlorpyrifos', 'Fipronil', 'Imidacloprid', 'Prometryn', 'Triclopyr']\n",
    "\n",
    "#regionIDs = ['BU','FI','CY','BM', 'WT', 'MW']\n",
    "#,'CY':'CY Rebuild 2015'\n",
    "#regionIDs = {'BU':'Burdekin Rebuild 2014','FI':'FI RC2019', 'WT':'WT_RC10'}\n",
    "\n",
    "#regionIDs = {'BM':'Burnett Mary Rebuild 2014','BU':'Burdekin Rebuild 2014','FI':'FI RC2019',\n",
    "#              'MW':'MW_RC10', 'WT':'WT_RC10'}\n",
    "\n",
    "\n",
    "regionIDs = {'BM':'Burnett Mary Rebuild 2014','BU':'Burdekin Rebuild 2014','FI':'FI RC2019',\n",
    "              'MW':'MW_RC10', 'WT':'WT_RC10', 'CY':'CY Rebuild 2015'}\n",
    "\n",
    "\n",
    "#After fixing inflow units\n",
    "#regionIDs = {'MW':'MW_RC10'}\n",
    "\n",
    "#elementMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\BurdekinElementMapper.csv'\n",
    "#mainPath = r'P:\\projects\\MichaelWarne\\Models\\results\\Burdekin Rebuild 2014\\BU_msPAF_Calibrated_503\\TimeSeries'\n",
    "#outPath = r'C:\\DDrive\\RC10PesticideProjs\\CSIROAggregatedOutlets\\Burdekin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outPath):\n",
    "    os.makedirs(outPath)\n",
    "    print(\"Made dir: \" + outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##This bit now needs to happen inside of loop\n",
    "# RiverElements = {}\n",
    "# elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "# for index, row in elementsTable.iterrows():\n",
    "#     if not row[riverCol] in RiverElements.keys():\n",
    "#         RiverElements[row[riverCol]] = []\n",
    "    \n",
    "#     RiverElements[row[riverCol]].append(row[elementCol])\n",
    "\n",
    "\n",
    "# #print(RiverElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allElementsTable = pd.read_csv(elementMapperCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDetailsDF = pd.read_csv(simMapperCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsPathInfo(regionIDString, scenarioString):\n",
    "    tsPath = ''\n",
    "    if addRegIdDIR:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDString + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    else:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    return tsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doThisSim(thisRegID, thisBasinFolderName, thisNewSimFolderName, thisExistSimFolderName, theseRiverElements):\n",
    "    \n",
    "    #Going to need to set up folders\n",
    "    #basinOutputStr = outPath + '\\\\' + thisBasinFolderName\n",
    "    basinOutputStr = os.path.join(outPath, thisBasinFolderName)\n",
    "    if not os.path.exists(basinOutputStr):\n",
    "        os.makedirs(basinOutputStr)\n",
    "        print(\"Made dir: \" + basinOutputStr)\n",
    "    \n",
    "    \n",
    "    simOutputStr = os.path.join(basinOutputStr, thisNewSimFolderName)\n",
    "    if not os.path.exists(simOutputStr):\n",
    "        os.makedirs(simOutputStr)\n",
    "        print(\"Made dir: \" + simOutputStr)    \n",
    "    \n",
    "    #Set up the totals first\n",
    "    RiverElements = {}\n",
    "    # elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "    for index, row in theseRiverElements.iterrows():\n",
    "         if not row[riverCol] in RiverElements.keys():\n",
    "            RiverElements[row[riverCol]] = []\n",
    "    \n",
    "         RiverElements[row[riverCol]].append(row[elementCol])\n",
    "    \n",
    "    riverTotals = {}\n",
    "    for theRiver in RiverElements.keys():\n",
    "        riverTotals[theRiver] = pd.DataFrame()\n",
    "\n",
    "    #Now data\n",
    "    mainPath = getResultsPathInfo(thisRegID, thisExistSimFolderName)\n",
    "\n",
    "    theFlowsDIR = os.path.join(mainPath,'Flows')\n",
    "    for theRiver in RiverElements.keys():\n",
    "        #We'll want flows first\n",
    "        eleCount = 0\n",
    "        flowFrame = pd.DataFrame()\n",
    "        outputFields = ['Date']\n",
    "        for theElement in RiverElements[theRiver]:\n",
    "            #This will stope Outlet Node1 matching Outle Node10, Outlet Node11\n",
    "            theElementUnder = theElement + '_'\n",
    "            for fname in os.listdir(theFlowsDIR):\n",
    "                if theElementUnder in fname:\n",
    "                    #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                    flowFilePath = os.path.join(theFlowsDIR,fname)\n",
    "                    flowData = pd.read_csv(flowFilePath)\n",
    "                    flowData.columns = ['Date', theElement]\n",
    "                    #riverTotals[theRiver].columns.drop(theElement)\n",
    "                    outputFields.append(theElement)\n",
    "                    if eleCount == 0:\n",
    "                        #Get the structure for our totals first\n",
    "                        riverTotals[theRiver] = flowData\n",
    "                        \n",
    "                        #Dropping this means we lose reference to the first (or only) named Element.\n",
    "                        #riverTotals[theRiver].drop(theElement, axis=1, inplace=True)\n",
    "                        #Now process stuff\n",
    "                        flowFrame = flowData\n",
    "                        #flowFrame.columns = ['Date', theElement]\n",
    "                        #flowFrame['Date'] = flowData['Date']\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values#This sould give the 2nd column\n",
    "                    else:\n",
    "                        #Join...??\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values\n",
    "                        #flowData will have flow col named after element, so won't be duplicated\n",
    "                        flowFrame = pd.merge(flowFrame, flowData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                    eleCount += 1\n",
    "        #Totals\n",
    "        #The .iloc[:, 1:] should exclude our first col, Date, from the sum\n",
    "        #flowFrame['Flow_cumecs'] = flowFrame.iloc[:, 1:].sum(axis=1)\n",
    "        flowFrame['Flow_cumecs'] = flowFrame.sum(axis=1)\n",
    "        #Write out flow\n",
    "        theOutFlowFile = os.path.join(simOutputStr, \"Flow \" + theRiver + \" cubicmetrespersecond.csv\")\n",
    "        if WriteIndividual:\n",
    "            flowFrame.to_csv(theOutFlowFile, index=False)\n",
    "        \n",
    "        #Need to join flow Total to RiverTotals\n",
    "        #This should be using ONLY 'Date' and 'Flow_cumecs', thereby dropping theElement from riverTotals... but is it???\n",
    "        #riverTotals[theRiver] = pd.merge(riverTotals[theRiver], flowFrame[['Date', 'Flow_cumecs']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "        #When doing Flow, we're creating these, so we should never hit the ELSE here, and vice versa foWQ\n",
    "        if len(riverTotals[theRiver].columns):\n",
    "            #print(\"We were empty doing: \" + theRiver)\n",
    "            riverTotals[theRiver] = flowFrame[['Date', 'Flow_cumecs']]\n",
    "        else:\n",
    "            #print(\"NOOOOOOTTTTT empty doing: \" + theRiver)\n",
    "            riverTotals[theRiver] = pd.merge(riverTotals[theRiver], flowFrame[['Date', 'Flow_cumecs']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "    #Now water quality\n",
    "    for theWQBit in Constituents:\n",
    "        wqPath = os.path.join(mainPath, theWQBit)\n",
    "        if not os.path.exists(wqPath):\n",
    "            continue\n",
    "    \n",
    "        for theRiver in RiverElements.keys():\n",
    "            #We'll want flows first\n",
    "            eleCount = 0\n",
    "            wqFrame = pd.DataFrame()\n",
    "            outputFields = ['Date']\n",
    "            for theElement in RiverElements[theRiver]:\n",
    "                theElementUnder = theElement + '_'\n",
    "                for fname in os.listdir(wqPath):\n",
    "                    if theElementUnder in fname:\n",
    "                        #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                        wqFilePath = os.path.join(wqPath,fname)\n",
    "                        wqData = pd.read_csv(wqFilePath)\n",
    "                        wqData.columns = ['Date', theElement]\n",
    "                        outputFields.append(theElement)\n",
    "                        if eleCount == 0:\n",
    "                            wqFrame = wqData\n",
    "                        else:\n",
    "                            wqFrame = pd.merge(wqFrame, wqData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                        eleCount += 1\n",
    "            #Totals\n",
    "            #wqFrame[theRiver + '_kg'] = wqFrame.iloc[:, 1:].sum(axis=1)\n",
    "            wqFrame[theRiver + '_kg'] = wqFrame.sum(axis=1)\n",
    "            #Write out flow\n",
    "            theOutFlowFile = os.path.join(simOutputStr, theWQBit + \" \" + theRiver + \" kilograms.csv\")\n",
    "            if WriteIndividual:\n",
    "                wqFrame.to_csv(theOutFlowFile, index=False)\n",
    "            \n",
    "            #Join to totals\n",
    "            #rename constituent total first\n",
    "            #print('theRiver: ' + theRiver)\n",
    "            #print('theWQBit: ' + theWQBit)\n",
    "            wqFrame.rename(columns={theRiver + '_kg':theWQBit + '_kg'}, inplace=True)\n",
    "            riverTotals[theRiver] = pd.merge(riverTotals[theRiver], wqFrame[['Date', theWQBit + '_kg']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "\n",
    "    #Now write totals files\n",
    "    for theRiver in RiverElements.keys():\n",
    "        theOutTotalsFile = os.path.join(simOutputStr, theRiver + \" Totals.csv\")\n",
    "        riverTotals[theRiver].to_csv(theOutTotalsFile, index=False)\n",
    "\n",
    "\n",
    "    print(\"Done \" + thisNewSimFolderName + \" at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing this simulation: WTBASE at 2022-06-01 16:05:33.740173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-deaaa3986cb0>:71: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  flowFrame['Flow_cumecs'] = flowFrame.sum(axis=1)\n",
      "<ipython-input-7-deaaa3986cb0>:65: FutureWarning: Passing 'suffixes' which cause duplicate columns {'TR03_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  flowFrame = pd.merge(flowFrame, flowData, how='left', left_on=['Date'], right_on=['Date'])\n",
      "<ipython-input-7-deaaa3986cb0>:65: FutureWarning: Passing 'suffixes' which cause duplicate columns {'TR02_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  flowFrame = pd.merge(flowFrame, flowData, how='left', left_on=['Date'], right_on=['Date'])\n",
      "<ipython-input-7-deaaa3986cb0>:116: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  wqFrame[theRiver + '_kg'] = wqFrame.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done WTBASE at 2022-06-01 16:05:49.385992\n",
      "Doing this simulation: WTPREDEV at 2022-06-01 16:05:49.385992\n",
      "Done WTPREDEV at 2022-06-01 16:06:10.460181\n",
      "Doing this simulation: MWBASE at 2022-06-01 16:06:10.460181\n",
      "Done MWBASE at 2022-06-01 16:06:31.511214\n",
      "Doing this simulation: MWPREDEV at 2022-06-01 16:06:31.520214\n",
      "Made dir: P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018\\MackayWhitsundays\\MWPREDEV\n",
      "Done MWPREDEV at 2022-06-01 16:06:59.517227\n",
      "Doing this simulation: CYBASE at 2022-06-01 16:06:59.532857\n",
      "Made dir: P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018\\CapeYork\n",
      "Made dir: P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018\\CapeYork\\CYBASE\n",
      "Done CYBASE at 2022-06-01 16:07:42.854733\n",
      "Doing this simulation: CYPREDEV at 2022-06-01 16:07:42.864826\n",
      "Made dir: P:\\projects\\RC10_ResultsSets\\CSIROAggregatedOutlets\\ReportCard2018\\CapeYork\\CYPREDEV\n",
      "Done CYPREDEV at 2022-06-01 16:08:26.650074\n",
      "All finished at 2022-06-01 16:08:26.650074\n"
     ]
    }
   ],
   "source": [
    "#This is where we will loop through sims\n",
    "for index, row in simDetailsDF.iterrows():\n",
    "    if row[regID] in regionIDs.keys():\n",
    "        print(\"Doing this simulation: \" + row[newSimFolderName] + \" at \" + str(datetime.datetime.now()))\n",
    "        #Filter out the river elements for this region\n",
    "        filteredElements = allElementsTable.loc[allElementsTable[regID] == row[regID]]\n",
    "        doThisSim(row[regID], row[basinFolderName], row[newSimFolderName], row[existSimFolderName], filteredElements)\n",
    "\n",
    "print(\"All finished at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
