{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "elementMapperCSV = r'P:\\projects\\eReefsOperational\\CSIRO_Locs\\CSIRO_GBR_ElementMapper.csv'\n",
    "#Names from element mapper\n",
    "regID = 'RegID'\n",
    "riverCol = 'Major River'\n",
    "elementCol = 'Network Element'\n",
    "\n",
    "#Use this to write out (or not) individual outlet files\n",
    "WriteIndividual = False\n",
    "\n",
    "#Standard eReefs baseline/pre-dev\n",
    "simMapperCSV = r'E:\\RC10PesticideProjects\\eReefs_Pesticides_ResultsMapper.csv'\n",
    "\n",
    "\n",
    "#Names from sim mapper, use RegID from above\n",
    "basinFolderName = 'BasinFolder'\n",
    "newSimFolderName = 'SIMCODE'\n",
    "#Switch these for TS Dependent or EMCMonthly\n",
    "#existSimFolderName = 'TSDependentSubDir'\n",
    "existSimFolderName = 'EMCMonthly2021SubDir'\n",
    "\n",
    "# #Switch these for parent location of TS Dependent or EMCMonthly results\n",
    "# #TSDependent\n",
    "# parentResultsDir = r'E:\\GBRFScenarios\\Results'\n",
    "# addRegIdDIR = True\n",
    "# outPath = r'E:\\GBRFScenarios\\CSIROAggregatedOutlets\\GBRDynSedNet'\n",
    "#Empirical\n",
    "parentResultsDir = r'E:\\RC10PesticideProjects\\results'\n",
    "addRegIdDIR = False\n",
    "outPath = r'E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021'\n",
    "\n",
    "riverOutlets = 'RiverOutlets'\n",
    "regAgg = 'RegionalAggregation'\n",
    "\n",
    "#, 'Sediment - Coarse'\n",
    "\n",
    "# #Constituents = ['Sediment - Fine', 'N_Particulate', 'N_DIN', 'N_DON', 'P_Particulate', 'P_DOP', 'P_FRP']\n",
    "Constituents = ['Ametryn', 'S-metolachlor', 'Atrazine', 'Diuron', 'Hexazinone', 'Tebuthiuron', '24-D'\n",
    "                , 'Paraquat', 'Glyphosate', 'Chlorsulfuron', 'Diquat', 'Fluroxypyr', 'Haloxyfop', 'Imazapic'\n",
    "               , 'Imazethapyr', 'Isoxaflutole', 'MCPA', 'Metribuzin', 'Metsulfuron-methyl', 'Pendimethalin', 'Simazine'\n",
    "               , 'Terbuthylazine', 'Trifluralin', 'Aciflurofen', 'Chlorpyrifos', 'Fipronil', 'Imidacloprid', 'Prometryn', 'Triclopyr']\n",
    "\n",
    "regionIDs = {'BM':'Burnett Mary Rebuild 2014','BU':'Burdekin Rebuild 2014','FI':'FI RC2019',\n",
    "              'MW':'MW_RC10', 'WT':'WT_RC10'}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outPath):\n",
    "    os.makedirs(outPath)\n",
    "    print(\"Made dir: \" + outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outPath + \"\\\\\" + riverOutlets):\n",
    "    os.makedirs(outPath + \"\\\\\" + riverOutlets)\n",
    "    print(\"Made dir: \" + outPath + \"\\\\\" + riverOutlets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outPath + \"\\\\\" + regAgg):\n",
    "    os.makedirs(outPath + \"\\\\\" + regAgg)\n",
    "    print(\"Made dir: \" + outPath + \"\\\\\" + regAgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RiverElements = {}\n",
    "# elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "# for index, row in elementsTable.iterrows():\n",
    "#     if not row[riverCol] in RiverElements.keys():\n",
    "#         RiverElements[row[riverCol]] = []\n",
    "    \n",
    "#     RiverElements[row[riverCol]].append(row[elementCol])\n",
    "\n",
    "\n",
    "# #print(RiverElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allElementsTable = pd.read_csv(elementMapperCSV)\n",
    "simDetailsDF = pd.read_csv(simMapperCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsPathInfoOutlets(regionIDString, scenarioString):\n",
    "    tsPath = ''\n",
    "    if addRegIdDIR:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDString + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    else:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\TimeSeries'\n",
    "    return tsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsPathInfoRegAgg(regionIDString, scenarioString):\n",
    "    tsPath = ''\n",
    "    if addRegIdDIR:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDString + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\Regional_TimeSeries\\\\Aggregated'\n",
    "    else:\n",
    "        tsPath = parentResultsDir + '\\\\' + regionIDs[regionIDString] + '\\\\' + scenarioString + '\\\\Regional_TimeSeries\\\\Aggregated'\n",
    "    return tsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doThisSimOutlet(thisRegID, thisBasinFolderName, thisNewSimFolderName, thisExistSimFolderName, theseRiverElements):\n",
    "    \n",
    "    #Going to need to set up folders\n",
    "    #basinOutputStr = outPath + '\\\\' + thisBasinFolderName\n",
    "    \n",
    "    outletPath = outPath + \"\\\\\" + riverOutlets\n",
    "    \n",
    "    basinOutputStr = os.path.join(outletPath, thisBasinFolderName)\n",
    "    if not os.path.exists(basinOutputStr):\n",
    "        os.makedirs(basinOutputStr)\n",
    "        print(\"Made dir: \" + basinOutputStr)\n",
    "    \n",
    "    \n",
    "    simOutputStr = os.path.join(basinOutputStr, thisNewSimFolderName)\n",
    "    if not os.path.exists(simOutputStr):\n",
    "        os.makedirs(simOutputStr)\n",
    "        print(\"Made dir: \" + simOutputStr)    \n",
    "    \n",
    "    #Set up the totals first\n",
    "    RiverElements = {}\n",
    "    # elementsTable = pd.read_csv(elementMapperCSV)\n",
    "\n",
    "    for index, row in theseRiverElements.iterrows():\n",
    "         if not row[riverCol] in RiverElements.keys():\n",
    "            RiverElements[row[riverCol]] = []\n",
    "    \n",
    "         RiverElements[row[riverCol]].append(row[elementCol])\n",
    "    \n",
    "    riverTotals = {}\n",
    "    for theRiver in RiverElements.keys():\n",
    "        riverTotals[theRiver] = pd.DataFrame()\n",
    "\n",
    "    #Now data\n",
    "    mainPath = getResultsPathInfoOutlets(thisRegID, thisExistSimFolderName)\n",
    "\n",
    "    theFlowsDIR = os.path.join(mainPath,'Flows')\n",
    "    for theRiver in RiverElements.keys():\n",
    "        #We'll want flows first\n",
    "        eleCount = 0\n",
    "        flowFrame = pd.DataFrame()\n",
    "        outputFields = ['Date']\n",
    "        for theElement in RiverElements[theRiver]:\n",
    "            #This will stope Outlet Node1 matching Outle Node10, Outlet Node11\n",
    "            theElementUnder = theElement + '_'\n",
    "            for fname in os.listdir(theFlowsDIR):\n",
    "                if theElementUnder in fname:\n",
    "                    #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                    flowFilePath = os.path.join(theFlowsDIR,fname)\n",
    "                    flowData = pd.read_csv(flowFilePath)\n",
    "                    flowData.columns = ['Date', theElement]\n",
    "                    #riverTotals[theRiver].columns.drop(theElement)\n",
    "                    outputFields.append(theElement)\n",
    "                    if eleCount == 0:\n",
    "                        #Get the structure for our totals first\n",
    "                        riverTotals[theRiver] = flowData\n",
    "                        \n",
    "                        #Dropping this means we lose reference to the first (or only) named Element.\n",
    "                        #riverTotals[theRiver].drop(theElement, axis=1, inplace=True)\n",
    "                        #Now process stuff\n",
    "                        flowFrame = flowData\n",
    "                        #flowFrame.columns = ['Date', theElement]\n",
    "                        #flowFrame['Date'] = flowData['Date']\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values#This sould give the 2nd column\n",
    "                    else:\n",
    "                        #Join...??\n",
    "                        #flowFrame[theElement] = flowData.iloc[:1].values\n",
    "                        #flowData will have flow col named after element, so won't be duplicated\n",
    "                        flowFrame = pd.merge(flowFrame, flowData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                    eleCount += 1\n",
    "        #Totals\n",
    "        #The .iloc[:, 1:] should exclude our first col, Date, from the sum\n",
    "        #flowFrame['Flow_cumecs'] = flowFrame.iloc[:, 1:].sum(axis=1)\n",
    "        flowFrame['Flow_cumecs'] = flowFrame.sum(axis=1)\n",
    "        #Write out flow\n",
    "        theOutFlowFile = os.path.join(simOutputStr, \"Flow \" + theRiver + \" cubicmetrespersecond.csv\")\n",
    "        if WriteIndividual:\n",
    "            flowFrame.to_csv(theOutFlowFile, index=False)\n",
    "        \n",
    "        #Need to join flow Total to RiverTotals\n",
    "        #This should be using ONLY 'Date' and 'Flow_cumecs', thereby dropping theElement from riverTotals... but is it???\n",
    "        #riverTotals[theRiver] = pd.merge(riverTotals[theRiver], flowFrame[['Date', 'Flow_cumecs']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "        #When doing Flow, we're creating these, so we should never hit the ELSE here, and vice versa foWQ\n",
    "        if len(riverTotals[theRiver].columns):\n",
    "            #print(\"We were empty doing: \" + theRiver)\n",
    "            riverTotals[theRiver] = flowFrame[['Date', 'Flow_cumecs']]\n",
    "        else:\n",
    "            #print(\"NOOOOOOTTTTT empty doing: \" + theRiver)\n",
    "            riverTotals[theRiver] = pd.merge(riverTotals[theRiver], flowFrame[['Date', 'Flow_cumecs']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "    #Now water quality\n",
    "    for theWQBit in Constituents:\n",
    "        wqPath = os.path.join(mainPath, theWQBit)\n",
    "        if not os.path.exists(wqPath):\n",
    "            continue\n",
    "    \n",
    "        for theRiver in RiverElements.keys():\n",
    "            #We'll want flows first\n",
    "            eleCount = 0\n",
    "            wqFrame = pd.DataFrame()\n",
    "            outputFields = ['Date']\n",
    "            for theElement in RiverElements[theRiver]:\n",
    "                theElementUnder = theElement + '_'\n",
    "                for fname in os.listdir(wqPath):\n",
    "                    if theElementUnder in fname:\n",
    "                        #print(\"For this river: \" + theRiver + \" we have this file: \" + fname)\n",
    "                        wqFilePath = os.path.join(wqPath,fname)\n",
    "                        wqData = pd.read_csv(wqFilePath)\n",
    "                        wqData.columns = ['Date', theElement]\n",
    "                        outputFields.append(theElement)\n",
    "                        if eleCount == 0:\n",
    "                            wqFrame = wqData\n",
    "                        else:\n",
    "                            wqFrame = pd.merge(wqFrame, wqData, how='left', left_on=['Date'], right_on=['Date'])\n",
    "                \n",
    "                        eleCount += 1\n",
    "            #Totals\n",
    "            #wqFrame[theRiver + '_kg'] = wqFrame.iloc[:, 1:].sum(axis=1)\n",
    "            wqFrame[theRiver + '_kg'] = wqFrame.sum(axis=1)\n",
    "            #Write out flow\n",
    "            theOutFlowFile = os.path.join(simOutputStr, theWQBit + \" \" + theRiver + \" kilograms.csv\")\n",
    "            if WriteIndividual:\n",
    "                wqFrame.to_csv(theOutFlowFile, index=False)\n",
    "            \n",
    "            #Join to totals\n",
    "            #rename constituent total first\n",
    "            #print('theRiver: ' + theRiver)\n",
    "            #print('theWQBit: ' + theWQBit)\n",
    "            wqFrame.rename(columns={theRiver + '_kg':theWQBit + '_kg'}, inplace=True)\n",
    "            riverTotals[theRiver] = pd.merge(riverTotals[theRiver], wqFrame[['Date', theWQBit + '_kg']], how='left', left_on=['Date'], right_on=['Date'])\n",
    "\n",
    "\n",
    "    #Now write totals files\n",
    "    for theRiver in RiverElements.keys():\n",
    "        theOutTotalsFile = os.path.join(simOutputStr, theRiver + \" Totals.csv\")\n",
    "        riverTotals[theRiver].to_csv(theOutTotalsFile, index=False)\n",
    "\n",
    "\n",
    "    print(\"Done \" + thisNewSimFolderName + \" at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doThisSimRegAgg(thisRegID, thisBasinFolderName, thisNewSimFolderName, thisExistSimFolderName):\n",
    "    ##fvggg\n",
    "    \n",
    "    #Going to need to set up folders\n",
    "    #basinOutputStr = outPath + '\\\\' + thisBasinFolderName\n",
    "    regAggPath = outPath + \"\\\\\" + regAgg\n",
    "    simOutputStr = os.path.join(regAggPath, thisNewSimFolderName)\n",
    "    if not os.path.exists(simOutputStr):\n",
    "        os.makedirs(simOutputStr)\n",
    "        print(\"Made dir: \" + simOutputStr)\n",
    "    \n",
    "    #Now data\n",
    "    mainPath = getResultsPathInfoRegAgg(thisRegID, thisExistSimFolderName)\n",
    "    \n",
    "    #copy\n",
    "    for name in glob.glob(mainPath + '/*.csv'):\n",
    "        shutil.copy(name, simOutputStr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing this simulation: BUBASE at 2021-12-23 14:28:52.657397\n",
      "Done BUBASE at 2021-12-23 14:29:05.493457\n",
      "Doing this simulation: FIBASE at 2021-12-23 14:29:06.279836\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\Fitzroy\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\Fitzroy\\FIBASE\n",
      "Done FIBASE at 2021-12-23 14:29:25.156389\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RegionalAggregation\\FIBASE\n",
      "Doing this simulation: WTBASE at 2021-12-23 14:29:25.889246\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\WetTropics\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\WetTropics\\WTBASE\n",
      "Done WTBASE at 2021-12-23 14:29:45.322243\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RegionalAggregation\\WTBASE\n",
      "Doing this simulation: BMBASE at 2021-12-23 14:29:46.709665\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\BurnettMary\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\BurnettMary\\BMBASE\n",
      "Done BMBASE at 2021-12-23 14:30:05.374193\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RegionalAggregation\\BMBASE\n",
      "Doing this simulation: MWBASE at 2021-12-23 14:30:06.392026\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\MackayWhitsundays\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RiverOutlets\\MackayWhitsundays\\MWBASE\n",
      "Done MWBASE at 2021-12-23 14:30:23.531007\n",
      "Made dir: E:\\RC10PesticideProjects\\CSIRO_Outputs_Nov2021\\RegionalAggregation\\MWBASE\n",
      "All finished at 2021-12-23 14:30:24.182707\n"
     ]
    }
   ],
   "source": [
    "#This is where we will loop through sims\n",
    "for index, row in simDetailsDF.iterrows():\n",
    "    if row[regID] in regionIDs.keys():\n",
    "        print(\"Doing this simulation: \" + row[newSimFolderName] + \" at \" + str(datetime.datetime.now()))\n",
    "        #Filter out the river elements for this region\n",
    "        filteredElements = allElementsTable.loc[allElementsTable[regID] == row[regID]]\n",
    "        doThisSimOutlet(row[regID], row[basinFolderName], row[newSimFolderName], row[existSimFolderName], filteredElements)\n",
    "        doThisSimRegAgg(row[regID], row[basinFolderName], row[newSimFolderName], row[existSimFolderName])\n",
    "\n",
    "print(\"All finished at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
