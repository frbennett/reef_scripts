{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah de blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This tool will not ensure that pixels and boundaries align\n",
    "# That is the job of the user\n",
    "# Must be operating as dev account\n",
    "import pandas\n",
    "import numpy\n",
    "from osgeo import gdal, gdalconst\n",
    "from osgeo import ogr\n",
    "import gc\n",
    "import tempfile\n",
    "import datetime\n",
    "import dateutil\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import ipyparallel as ipp\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing\n",
    "sys.path.append('..\\\\Modules')\n",
    "import usle_cfactor_analysis\n",
    "#import zonal_stats_ForLayerObject\n",
    "import resample_raster\n",
    "#from rasterstats import zonal_stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Path to intersected Subcatch/FUs shapefile\n",
    "pathToShapefile = r\"C:\\DDrive\\MW\\v450projs\\SpatialExp2019\\ExportedCatchments-FUs Intersection.shp\"\n",
    "#pathToShapefile = \"D:/MW/spatial_RC7/mw_intscfu_clean.shp\"\n",
    "#pathToShapefile = \"D:/MW/spatial_RC7/badFUOnly.shp\"\n",
    "\n",
    "uniqueIDField = \"IntSCFU\"\n",
    "catchmentField = \"IntSCs\"\n",
    "FUField = \"IntFUs\"\n",
    "\n",
    "FUsToInclude = [\"Grazing Open\",\"Grazing Forested\",\"Forestry\",\"Conservation\"]\n",
    "\n",
    "#Path to template raster (probably subcatchments or DEM)\n",
    "rasterTemplateFile = r\"C:\\DDrive\\MW\\v450projs\\SpatialExp2019\\CreatedSubcats.asc\"\n",
    "\n",
    "#Output intersected Subcat/FU raster\n",
    "#Will get a CSV of same name written 'mapping' each FU to a unique ID (the unique ID will be the raster values)\n",
    "rasterOutputFile = r\"C:\\DDrive\\MW\\v450projs\\SpatialExp2019\\MW_USLE_IntSubCatFu.tif\"\n",
    "\n",
    "#Path to K raster\n",
    "rasterKFile = r\"C:\\DDrive\\MW\\v450projs\\Ram\\mw_kfact.asc\"\n",
    "\n",
    "#Path to LS raster\n",
    "rasterLSFile =r\"C:\\DDrive\\MW\\v450projs\\Ram\\mw_ls_fact.tif\"\n",
    "\n",
    "#Path to Fines raster\n",
    "rasterFinesFile = r\"C:\\DDrive\\MW\\v450projs\\Ram\\mw_cs_surf.asc\"\n",
    "\n",
    "#Path to Scald raster\n",
    "#rasterScaldFile = r\"C:\\DDrive\\MW\\v450projs\\Ram\\Exported Catchments Raster.asc\"\n",
    "\n",
    "#Path to Cfactor directory\n",
    "cFactorDIR = r\"C:\\DDrive\\MW\\v450projs\\Ram\\c-factors\"\n",
    "#cFactorDIR = \"D:\\MW\\Ram_USLE\\skip\\justone\"\n",
    "\n",
    "#Cfactor extension\n",
    "cFactExt = \"tif\"\n",
    "\n",
    "#cFactor Date naming pattern\n",
    "cFactPattern = \"xxxyyyymm\"\n",
    "\n",
    "monthToSeasonOffsets = {1:-1, 2:-2, 3:0, 4:-1, 5:-2, 6:0, 7:-1, 8:-2, 9:0, 10:-1, 11:-2, 12:0}\n",
    "\n",
    "#More Cfact settings\n",
    "defaultCfact = 0.05\n",
    "reqdValidProp = 0.20\n",
    "\n",
    "#declare a couple of variables\n",
    "IntSCFUStr = \"IntSCFU\"\n",
    "MeanKLSCStr = \"Mean_KLSC\"\n",
    "MeanKLSCFinesStr = \"Mean_KLSC_Fines\"\n",
    "MeanKStr = \"Mean_K\"\n",
    "MeanFinesStr = \"Mean_Fines\"\n",
    "MeanCfactStr = \"Mean_CFact\"\n",
    "MeanLSStr = \"Mean_LS\"\n",
    "MeanScaldStr = \"Mean_Scald\"\n",
    "AreaStr = \"Area\"\n",
    "TimeStampStr = \"Time_Stamp\"\n",
    "CellCountStr = \"Cell_Count\"\n",
    "\n",
    "#Directory and filename constants for USLE parameterisation\n",
    "USLE_DIR_KLSC = \"KLSC\";\n",
    "USLE_DIR_KLSCFINE = \"KLSC_Fines\";\n",
    "USLE_DIR_KLSC_CFACT = \"Cfact\";\n",
    "USLE_FILE_KLSC = \"USLE_KLSC_Total\";\n",
    "USLE_FILE_KLSCFINES = \"USLE_KLSC_FinePerc\";\n",
    "USLE_FILE_KLS_CFACT = \"CFactor\";\n",
    "\n",
    "rasterResultsDF = pandas.DataFrame(columns=[IntSCFUStr,TimeStampStr,MeanKLSCStr,MeanKLSCFinesStr,MeanKStr,MeanCfactStr,MeanFinesStr,MeanLSStr,MeanScaldStr,AreaStr,CellCountStr])\n",
    "#rasterResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2788, 1: 6660, 2: 13808, 3: 19056, 4: 17976, 5: 17948}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### as dev.ellisrj, start ipcluster in Anaconda prompt:\n",
    "###> ipcluster start -n 6\n",
    "\n",
    "rc = ipp.Client()\n",
    "ar = rc[:].apply_async(os.getpid)\n",
    "pid_map = ar.get_dict()\n",
    "pid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677868.63051, 30.0, 0.0, -2257960.4690071, 0.0, -30.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the template raster\n",
    "templateRas = gdal.Open(rasterTemplateFile)\n",
    "template_GeoTrans = templateRas.GetGeoTransform()\n",
    "template_GeoTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672918.63051, 30.0, 0.0, -2251120.4690071, 0.0, -30.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load K factor\n",
    "kFactRas = gdal.Open(rasterKFile)\n",
    "kfact_GeoTrans = kFactRas.GetGeoTransform()\n",
    "kfact_GeoTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfactor raster resolution info didn't match template, attempted a resample\n"
     ]
    }
   ],
   "source": [
    "if (kfact_GeoTrans == template_GeoTrans):\n",
    "    print(\"Kfactor raster resolution info matches template, excellent\")\n",
    "else:\n",
    "    print(\"Kfactor raster resolution info didn't match template, attempted a resample\")\n",
    "    kFactRas = resample_raster.resample_raster_to_match_template(templateRas, kFactRas, os.path.join(tempfile.gettempdir(), \"resampledKFact.tif\"))\n",
    "    #kFactRas = resample_raster_to_match_template(templateRas, kFactRas, \"D:/aa/tester.tif\")\n",
    "    kfact_GeoTrans = kFactRas.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfactor raster resolution info now matches template, excellent\n"
     ]
    }
   ],
   "source": [
    "if (kfact_GeoTrans == template_GeoTrans):\n",
    "    print(\"Kfactor raster resolution info now matches template, excellent\")\n",
    "    #kFactRas = None\n",
    "    #kFactRas = gdal.Open(os.path.join(tempfile.gettempdir(), \"resampledKFact.tif\"))\n",
    "    #kfact_GeoTrans = kFactRas.GetGeoTransform()\n",
    "else:\n",
    "    print(\"Kfactor raster resolution still not the same as the template, you shouldn't proceed\")\n",
    "#zone_ds = None\n",
    "#print(\"rows: \" + str(kFactRas.RasterYSize) + \" cols: \" + str(kFactRas.RasterXSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9999.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kFactRas.GetRasterBand(1).GetNoDataValue()\n",
    "templateRas.GetRasterBand(1).GetNoDataValue()\n",
    "\n",
    "# The line below would get the kfactor raster written to disk, but it may not be necessary\n",
    "#kFactRas = None\n",
    "\n",
    "#this stuf to draw up a sample of KFactor, have to chop off heaps of edges so that the range in the colorbar doesn't consider -9999\n",
    "#kFactRasArray = kFactRas.ReadAsArray()\n",
    "#array_chop = kFactRasArray[3500:-3500,1500:-1500]  # chop off the outer specified pixels\n",
    "#plt.imshow(array_chop, interpolation='nearest')\n",
    "#plt.colorbar()\n",
    "#plt.title('K Factor')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load LS factor\n",
    "lsFactRas = gdal.Open(rasterLSFile)\n",
    "lsFact_GeoTrans = lsFactRas.GetGeoTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSfactor raster resolution info didn't match template, attempted a resample\n"
     ]
    }
   ],
   "source": [
    "if (lsFact_GeoTrans == template_GeoTrans):\n",
    "    print(\"LSfactor raster resolution info matches template, excellent\")\n",
    "else:\n",
    "    print(\"LSfactor raster resolution info didn't match template, attempted a resample\")\n",
    "    lsFactRas = resample_raster.resample_raster_to_match_template(templateRas, lsFactRas, os.path.join(tempfile.gettempdir(), \"resampledLSFact.tif\"))\n",
    "    lsFact_GeoTrans = lsFactRas.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSfactor raster resolution info now matches template, excellent\n"
     ]
    }
   ],
   "source": [
    "if (lsFact_GeoTrans == template_GeoTrans):\n",
    "    print(\"LSfactor raster resolution info now matches template, excellent\")\n",
    "    #lsFactRas = None\n",
    "    #lsFactRas = gdal.Open(os.path.join(tempfile.gettempdir(), \"resampledLSFact.tif\"))\n",
    "    #lsfact_GeoTrans = lsFactRas.GetGeoTransform()\n",
    "else:\n",
    "    print(\"LSfactor raster resolution still not the same as the template, you shouldn't proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672918.63051, 30.0, 0.0, -2251120.4690071, 0.0, -30.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Fines raster\n",
    "FinesRas = gdal.Open(rasterFinesFile)\n",
    "fines_GeoTrans = FinesRas.GetGeoTransform()\n",
    "fines_GeoTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fines raster resolution info didn't match template, attempted a resample\n"
     ]
    }
   ],
   "source": [
    "if (fines_GeoTrans == template_GeoTrans):\n",
    "    print(\"Fines raster resolution info matches template, excellent\")\n",
    "else:\n",
    "    print(\"Fines raster resolution info didn't match template, attempted a resample\")\n",
    "    FinesRas = resample_raster.resample_raster_to_match_template(templateRas, FinesRas, os.path.join(tempfile.gettempdir(), \"resampledFines.tif\"))\n",
    "    fines_GeoTrans = FinesRas.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fines raster resolution info now matches template, excellent\n"
     ]
    }
   ],
   "source": [
    "if (fines_GeoTrans == template_GeoTrans):\n",
    "    print(\"Fines raster resolution info now matches template, excellent\")\n",
    "    #FinesRas = None\n",
    "    #FinesRas = gdal.Open(os.path.join(tempfile.gettempdir(), \"resampledFines.tif\"))\n",
    "    #fines_GeoTrans = FinesRas.GetGeoTransform()\n",
    "else:\n",
    "    print(\"Fines raster resolution info still not the same as the template, you shouldn't proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1399"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open shapefile\n",
    "basePolys = ogr.Open(pathToShapefile)\n",
    "type(basePolys)\n",
    "nlay = basePolys.GetLayerCount()\n",
    "lyr = basePolys.GetLayer(0)\n",
    "ext = lyr.GetExtent()\n",
    "#nlay\n",
    "#ext\n",
    "lyr.GetFeatureCount()\n",
    "#plt.imshow(lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "featCount = 0\n",
    "for feat in lyr:\n",
    "    ##print(\"Doing something\")\n",
    "    #print(feat.GetFieldAsString(catchmentField) + \" and FU \" + feat.GetFieldAsString(FUField))\n",
    "    featCount += 1\n",
    "    if (featCount == 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Grazing Open','Grazing Forested','Forestry','Conservation'\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUsToInclude = [\"Grazing Open\",\"Grazing Forested\",\"Forestry\"]\n",
    "\n",
    "FUsAsSQL = \"\"\n",
    "count = 0\n",
    "for thisFU in FUsToInclude:\n",
    "    if(count == 0):\n",
    "        FUsAsSQL = \"'\" + thisFU + \"'\"\n",
    "    else:\n",
    "        FUsAsSQL += \",'\" + thisFU + \"'\"\n",
    "    count += 1\n",
    "\n",
    "FUsAsSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyr.SetAttributeFilter( FUField + \" IN (\" + FUsAsSQL + \")\")\n",
    "lyr.GetFeatureCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = rasterOutputFile.replace(\".tif\",\".csv\")\n",
    "fumapper = pandas.DataFrame(columns=[uniqueIDField, catchmentField, FUField])\n",
    "#featCount = 0\n",
    "for feat in lyr:\n",
    "    #print(feat.GetFieldAsString(catchmentField) + \" and FU \" + feat.GetFieldAsString(FUField))\n",
    "    #fumapper.append([feat.GetFieldAsString(uniqueIDField),feat.GetFieldAsString(catchmentField),feat.GetFieldAsString(FUField)])\n",
    "    newList = [feat.GetFieldAsString(uniqueIDField),feat.GetFieldAsString(catchmentField),feat.GetFieldAsString(FUField)]\n",
    "    fumapper.loc[len(fumapper)] = newList\n",
    "    #featCount += 1\n",
    "    #if (featCount == 20):\n",
    "    #    break\n",
    "\n",
    "#lyr.GetFeatureCount()\n",
    "#fumapper.to_csv(csvFile, index=False)\n",
    "#fumapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##help(templateRas)\n",
    "geo_transform = templateRas.GetGeoTransform()\n",
    "x_min = geo_transform[0]\n",
    "y_max = geo_transform[3]\n",
    "x_max = x_min + geo_transform[1] * templateRas.RasterXSize\n",
    "y_min = y_max + geo_transform[5] * templateRas.RasterYSize\n",
    "x_count = templateRas.RasterXSize\n",
    "y_count = templateRas.RasterYSize\n",
    "pixel_width = geo_transform[1]\n",
    "pixel_height = geo_transform[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_ds = gdal.GetDriverByName(\"GTiff\").Create(rasterOutputFile, x_count, y_count, 1, gdal.GDT_Int32)\n",
    "##Doing MEM did not yield the expected unique vals\n",
    "##zone_ds = gdal.GetDriverByName(\"MEM\").Create(\"\", x_count, y_count, 1, gdal.GDT_Byte)\n",
    "# This was shifting the resulting raster northwards in our Albers stuff\n",
    "#zone_ds.SetGeoTransform((x_min, pixel_width, 0, y_min, 0, pixel_width))\n",
    "zone_ds.SetGeoTransform((x_min, pixel_width, 0, y_max, 0, pixel_height))\n",
    "band = zone_ds.GetRasterBand(1)\n",
    "NoData_value = -9999\n",
    "band.SetNoDataValue(NoData_value)\n",
    "band.FlushCache()\n",
    "gdal.RasterizeLayer(zone_ds, [1], lyr, options=[\"ATTRIBUTE=\" + uniqueIDField])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This bit essential to get the raster file written - but could we use it before writing out (and then having to read back in?)\n",
    "##Hopefully not needed if MEM type\n",
    "##zone_ds = None\n",
    "#type(zone_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zone_ds.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonesRasterArray = zone_ds.ReadAsArray()\n",
    "zoneMask = numpy.ma.masked_where(zonesRasterArray == NoData_value, zonesRasterArray)\n",
    "#uniq = numpy.unique(zonesRasterArray)\n",
    "##print(uniq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#type(lyr)\n",
    "#stats = zonal_stats(lyr, kFactRas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#K factor zonal stats\n",
    "#Kfact_stats = zonal_stats_ForLayerObject.zonal_stats(lyr, uniqueIDField, kFactRas, -9999)\n",
    "# Will get 'converting a masked element to nan' error for tiny polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load LS factor zonal stats\n",
    "#LSfact_stats = zonal_stats_ForLayerObject.zonal_stats(lyr, uniqueIDField, lsFactRas, -9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fines raster zonal stats\n",
    "#Fines_stats = zonal_stats_ForLayerObject.zonal_stats(lyr, uniqueIDField, FinesRas, -9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kFactRas = None\n",
    "#lsFactRas = None\n",
    "#FinesRas = None\n",
    "\n",
    "#index = 0\n",
    "#while index < len(Kfact_stats):\n",
    "#    if(Kfact_stats[index][uniqueIDField] == 1199):\n",
    "#        print(str(Kfact_stats[index][uniqueIDField]) + \" KFactor Mean: \" + str(Kfact_stats[index]['mean']) + \" Count: \" + str(Kfact_stats[index]['count']))\n",
    "#        print(str(LSfact_stats[index][uniqueIDField]) + \" LS Mean: \" + str(LSfact_stats[index]['mean']) + \" Count: \" + str(LSfact_stats[index]['count']))\n",
    "#        print(str(Fines_stats[index][uniqueIDField]) + \" Fines Mean: \" + str(Fines_stats[index]['mean']) + \" Count: \" + str(Fines_stats[index]['count']))\n",
    "#        break\n",
    "    \n",
    "#    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now need to combine K, LS & Fines\n",
    "\n",
    "kFactNumpy = kFactRas.GetRasterBand(1).ReadAsArray()\n",
    "lsFactNumpy = lsFactRas.GetRasterBand(1).ReadAsArray()\n",
    "FinesNumpy = FinesRas.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "kFactMasked = numpy.ma.masked_where(kFactNumpy == kFactRas.GetRasterBand(1).GetNoDataValue(), kFactNumpy)\n",
    "lsFactMasked = numpy.ma.masked_where(lsFactNumpy == lsFactRas.GetRasterBand(1).GetNoDataValue(), lsFactNumpy)\n",
    "FinesMasked = numpy.ma.masked_where(FinesNumpy == FinesRas.GetRasterBand(1).GetNoDataValue(), FinesNumpy)\n",
    "\n",
    "\n",
    "\n",
    "kFactRas = None\n",
    "lsFactRas = None\n",
    "FinesRas = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.072e-05\n"
     ]
    }
   ],
   "source": [
    "#what will this do for nodata?\n",
    "# it will give -9999 ^ 3. Need to mask!\n",
    "KLSFinesNumpy = kFactMasked * lsFactMasked * (FinesMasked / 100)\n",
    "KLSNumpy = kFactMasked * lsFactMasked\n",
    "print(KLSFinesNumpy.min())\n",
    "#print(KLSFinesNumpy.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677868.63051, 30.0, 0.0, -2257960.4690071, 0.0, -30.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fines_GeoTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1678633.63051"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XCoordMid = fines_GeoTrans[0] + (25 * fines_GeoTrans[1]) + (fines_GeoTrans[1] / 2)\n",
    "XCoordMid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2258725.4690071"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YCoordMid = fines_GeoTrans[3] + (25 * fines_GeoTrans[5]) + (fines_GeoTrans[5] / 2)\n",
    "YCoordMid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theRowForX = (1678633.63051 - fines_GeoTrans[0] - (fines_GeoTrans[1] / 2)) / fines_GeoTrans[1]\n",
    "theRowForX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theColForY = (-2258725.4690071 - fines_GeoTrans[3] - (fines_GeoTrans[5] / 2)) / fines_GeoTrans[5]\n",
    "theColForY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centreCoordsForPixel(gt, row, col):\n",
    "    \n",
    "    XCoordMid = gt[0] + (col * gt[1]) + (gt[1] / 2)\n",
    "    YCoordMid = gt[3] + (row * gt[5]) + (gt[5] / 2)\n",
    "    \n",
    "    return (XCoordMid, YCoordMid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelForCentreCoords(gt, xCoord, yCoord):\n",
    "    \n",
    "    yCol = int((xCoord - gt[0] - (gt[1] / 2)) / gt[1])\n",
    "    xRow = int((yCoord - gt[3] - (gt[5] / 2)) / gt[5])\n",
    "    \n",
    "    return(xRow, yCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = pixelForCentreCoords(fines_GeoTrans, 1678633.63051, -2258725.4690071)\n",
    "tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2258965.4690071"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = centreCoordsForPixel(fines_GeoTrans, 33, 66)\n",
    "test2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677868.63051, 30.0, 0.0, -2257960.4690071, 0.0, -30.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_ds.GetGeoTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDateFromFileName(theFileName, pattern):\n",
    "    \n",
    "    allowedChars = [\"x\", \"y\", \"m\", \"d\"]\n",
    "    \n",
    "    for cha in pattern:\n",
    "        if cha not in allowedChars:\n",
    "            print(\"You can't use '\" + cha + \"' ya dill\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    if \"yyyy\" not in pattern:\n",
    "        print(\"You've got to at least have 'yyyy' in your pattern\")\n",
    "        return\n",
    "    \n",
    "    Day = 1\n",
    "    Month = 1\n",
    "    Year = int(theFileName[pattern.index(\"yyyy\"): pattern.index(\"yyyy\") + 4])\n",
    "    \n",
    "    if \"mm\" in pattern:\n",
    "        Month = int(theFileName[pattern.index(\"mm\"): pattern.index(\"mm\") + 2])\n",
    "    \n",
    "   \n",
    "    if \"dd\" in pattern:\n",
    "        Day = int(theFileName[pattern.index(\"dd\"): pattern.index(\"dd\") + 2])\n",
    "        \n",
    "\n",
    "    boogity = datetime.date(Year, Month, Day)\n",
    "    \n",
    "    return boogity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2016, 3, 1), datetime.date(2016, 6, 1), datetime.date(2016, 9, 1), datetime.date(2016, 12, 1), datetime.date(2017, 3, 1), datetime.date(2017, 6, 1), datetime.date(2017, 9, 1), datetime.date(2017, 12, 1), datetime.date(2018, 3, 1), datetime.date(2018, 6, 1)]\n"
     ]
    }
   ],
   "source": [
    "# list out Cfactor files\n",
    "#cFactorDIR = \"D:\\MW\\Ram_USLE\\skip\"\n",
    "\n",
    "cRastersInfo = {}\n",
    "cRastersAlone = []\n",
    "for file in os.listdir(cFactorDIR):\n",
    "    if file.endswith(\".\" + cFactExt):\n",
    "        #print(os.path.join(cFactorDIR, file) + \" and \" + str(getDateFromFileName(file, \"xxxyyyymm\")))\n",
    "        #cRastersInfo.append(getDateFromFileName(file, \"xxxyyyymm\"), os.path.join(cFactorDIR, file))\n",
    "        cRastersInfo[getDateFromFileName(file, cFactPattern)] =  os.path.join(cFactorDIR, file)\n",
    "        cRastersAlone.append(os.path.join(cFactorDIR, file))\n",
    "\n",
    "#cRastersInfo = sorted(cRastersInfo, reverse=True)\n",
    "cRastersInfoKeysSorted = sorted(cRastersInfo)\n",
    "print(cRastersInfoKeysSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkdate = cRastersInfoKeysSorted[0]\n",
    "\n",
    "#print(cRastersInfo.get(checkdate))\n",
    "\n",
    "#print(str(zonesRasterArray[100,100]))\n",
    "#print(str(KLSNumpy[100,100]))\n",
    "#print(str(zonesRasterArray[1000,1000]))\n",
    "#print(str(NoData_value))\n",
    "\n",
    "#Need to re-fill the masked KLS arrays so that we can detect 'nodata' cells\n",
    "#mostly where our coastal mask is missing a slope cell or two\n",
    "KLSNumpyFilled = numpy.ma.filled(KLSNumpy, NoData_value)\n",
    "KLSFinesNumpyFilled = numpy.ma.filled(KLSFinesNumpy, NoData_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoneIDStr = \"zoneID\"\n",
    "zoneCountStr = \"zoneCount\"\n",
    "validKLSCSum = \"validKLSCSum\"\n",
    "allKLSCSum = \"allKLSCSum\"\n",
    "validKLSCFinesSum = \"validKLSCFinesSum\"\n",
    "allKLSCFinesSum = \"allKLSCFinesSum\"\n",
    "validDataCountStr = \"dataCount\"\n",
    "\n",
    "KLSCvalStr = \"KLSC\"\n",
    "KLSCFinesValStr = \"KLSCFines\"\n",
    "\n",
    "def analyseThisCFactorRaster(theCFactorRas, datePattern, theZonesRasterArray, zonesGT, \n",
    "                             ZonesRowCount, ZonesColCount, ZoneND, theDefaultC, theReqValidProp, \n",
    "                             theKLSNumpy, theKLSFinesNumpy):\n",
    "    # do pixel by pixel analysis\n",
    "    # zones raster of intersected FUs will exist, and KLSFines raster will be co-located with this\n",
    "    # send back a dictionary that also includes the valid/invalid cFactor numbers for each FU\n",
    "    # should be able to access rasters etc without passing them into this function\n",
    "    # same for the dictionary of results, it could already exist\n",
    "    \n",
    "    # Will need to watch the Y min/max interpretation for our Aus Albers stuff\n",
    "    zoneIDStr = \"zoneID\"\n",
    "    zoneCountStr = \"zoneCount\"\n",
    "    validKLSCSum = \"validKLSCSum\"\n",
    "    allKLSCSum = \"allKLSCSum\"\n",
    "    validKLSCFinesSum = \"validKLSCFinesSum\"\n",
    "    allKLSCFinesSum = \"allKLSCFinesSum\"\n",
    "    validDataCountStr = \"dataCount\"\n",
    "    KLSCvalStr = \"KLSC\"\n",
    "    KLSCFinesValStr = \"KLSCFines\"\n",
    "    \n",
    "    theRepDate = getDateFromFileName(os.path.basename(theCFactorRas), datePattern)\n",
    "\n",
    "        \n",
    "    cFactRas = gdal.Open(theCFactorRas)\n",
    "    cFact_GeoTrans = cFactRas.GetGeoTransform()\n",
    "    #print(cFact_GeoTrans)\n",
    "    cFactND = cFactRas.GetRasterBand(1).GetNoDataValue()\n",
    "    cFactNumpy = cFactRas.ReadAsArray()\n",
    "    \n",
    "    #print(\"CFactND: \" + str(cFactND))\n",
    "    \n",
    "    cFactRows = cFactRas.RasterYSize\n",
    "    cFactCols = cFactRas.RasterXSize\n",
    "    \n",
    "    #zGT = theZones.GetGeoTransform()\n",
    "    zGT = zonesGT\n",
    "    \n",
    "    #theZonesRasterArray = theZones.ReadAsArray()\n",
    "    \n",
    "\n",
    "    statsDict = {}\n",
    "    statsDictToReturn = {}\n",
    "    uniq = numpy.unique(theZonesRasterArray)\n",
    "    for num in uniq:\n",
    "        #using masked, so this check for NoData should be redundant\n",
    "        #print(\"trying this UID: \" + str(num))\n",
    "        if num == ZoneND:\n",
    "            continue\n",
    "        \n",
    "        statsDict[num] = {zoneCountStr: int(0), validKLSCSum: float(0), validKLSCFinesSum: float(0),\n",
    "                          allKLSCSum: float(0), allKLSCFinesSum: float(0), validDataCountStr: int(0)}\n",
    "\n",
    "    rowid = 0\n",
    "    while rowid < ZonesRowCount:\n",
    "    #while rowid < theZones.RasterYSize:\n",
    "        #reset colid\n",
    "        colid = 0\n",
    "        #if rowid % 500 == 0:\n",
    "        #    print(\"Starting row: \" + str(rowid))\n",
    "        \n",
    "        while colid < ZonesColCount:\n",
    "        #while colid < theZones.RasterXSize:\n",
    "            zoneVal = theZonesRasterArray[rowid, colid]\n",
    "            \n",
    "            if zoneVal == ZoneND:\n",
    "                colid+=1\n",
    "                continue\n",
    "            \n",
    "            #we have explicitly ensured that KLSFines matches our pixels\n",
    "            #although we will have filled the mask with NoData vals\n",
    "            #so just need to get the requisite cFactor row/col\n",
    "            \n",
    "            KLS = theKLSNumpy[rowid, colid]\n",
    "            if KLS == ZoneND:\n",
    "                colid+=1\n",
    "                continue\n",
    "            \n",
    "            KLSFines = theKLSFinesNumpy[rowid, colid]\n",
    "            if KLSFines == ZoneND:\n",
    "                colid+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            zoneCoords = centreCoordsForPixel(zGT, rowid, colid)\n",
    "            #print(\"Zone Coords: \" + str(zoneCoords))\n",
    "            cFactPix = pixelForCentreCoords(cFact_GeoTrans, zoneCoords[0], zoneCoords[1])\n",
    "            #print(\"cFactPix: \" + str(cFactPix))\n",
    "            \n",
    "            #if cFactPix[0] >= cFactRows:\n",
    "            #    print(\"Exceeded row count at \" + str(cFactPix[0]) + \" for coords: \" + str(zoneCoords)\n",
    "            #          + \" where zone pixels were: \" + str(rowid) + \", \" + str(colid))\n",
    "            #    SystemExit(0)\n",
    "            \n",
    "            #if cFactPix[1] >= cFactCols:\n",
    "            #    print(\"Exceeded col count \" + str(cFactPix[1]) + \" for coords: \" + str(zoneCoords)\n",
    "            #          + \" where zone pixels were: \" + str(rowid) + \", \" + str(colid))\n",
    "            #    SystemExit(0)\n",
    "            \n",
    "            theC = theDefaultC\n",
    "            \n",
    "            isValid = False\n",
    "            cRasVal = cFactNumpy[cFactPix[0], cFactPix[1]]\n",
    "            if not cRasVal == cFactND:\n",
    "                theC = cRasVal\n",
    "                isValid = True\n",
    "            \n",
    "            KLSC = KLS * theC\n",
    "            KLSCFines = KLSFines * theC\n",
    "            \n",
    "            #if(KLSC < 0):\n",
    "            #    print(\"Badness at \"  + str(rowid) + \", \" + str(colid) + \" where KLS was: \" \n",
    "            #          + str(theKLSNumpy[rowid, colid]) + \" and C was: \" + str(theC))\n",
    "            #    SystemExit(0)\n",
    "            \n",
    "            statsDict[zoneVal][zoneCountStr] += 1\n",
    "            statsDict[zoneVal][allKLSCSum] += KLSC\n",
    "            statsDict[zoneVal][allKLSCFinesSum] += KLSCFines\n",
    "            \n",
    "            if(isValid):\n",
    "                statsDict[zoneVal][validKLSCSum] += KLSC\n",
    "                statsDict[zoneVal][validKLSCFinesSum] += KLSCFines\n",
    "                statsDict[zoneVal][validDataCountStr] += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            colid += 1\n",
    "        \n",
    "        \n",
    "        rowid += 1\n",
    "        \n",
    "        \n",
    "    #now work out which stats to use\n",
    "    #look to see if we'll use the default Cfactor derived values in lieu of valid entries\n",
    "    for uid in statsDict.keys():\n",
    "        \n",
    "        #if uid == 34:\n",
    "        #    print(statsDict[uid])\n",
    "        \n",
    "        zoneCount = statsDict[uid][zoneCountStr]\n",
    "        if zoneCount == 0:\n",
    "            #nothing to report, add zeros\n",
    "            statsDictToReturn[uid] = {KLSCvalStr: -9999, KLSCFinesvalStr: -9999}\n",
    "            continue\n",
    "        \n",
    "        valProp = 0\n",
    "        valCount = statsDict[uid][validDataCountStr]\n",
    "        if(valCount > 0):\n",
    "            valProp = valCount / zoneCount\n",
    "         \n",
    "        if valProp >= theReqValidProp:\n",
    "            # use valid only stats\n",
    "            statsDictToReturn[uid] = {KLSCvalStr: float(statsDict[uid][validKLSCSum] / valCount),\n",
    "                                      KLSCFinesValStr: float(statsDict[uid][validKLSCFinesSum] / valCount)}\n",
    "        else:\n",
    "            # Use stats that include the default where necessary\n",
    "            statsDictToReturn[uid] = {KLSCvalStr: float(statsDict[uid][allKLSCSum] / zoneCount),\n",
    "                                      KLSCFinesValStr: float(statsDict[uid][allKLSCFinesSum] / zoneCount)}\n",
    "        \n",
    "    \n",
    "    statsWithDate = {}\n",
    "    statsWithDate[theRepDate] = statsDictToReturn\n",
    "    \n",
    "    theCFatcorRas = None\n",
    "    theZonesRasterArray = None\n",
    "    zonesGT = None\n",
    "    ZonesRowCount =  None\n",
    "    ZonesColCount = None\n",
    "    ZoneND = None\n",
    "    theDefaultC = None\n",
    "    theReqValidProp = None\n",
    "    theKLSNumpy = None\n",
    "    theKLSFinesNumpy = None\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return statsWithDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "\n",
    "\n",
    "##tester = analyseThisCFactorRaster(r\"D:\\\\MW\\\\Ram_USLE\\\\skip\\\\justone\\\\cf_198712_198802.tif\", \n",
    "##                                  zonesRasterArray, zone_ds.GetGeoTransform(), \n",
    "##                                           zone_ds.RasterYSize, zone_ds.RasterXSize, NoData_value, defaultCfact, \n",
    "##                                           reqdValidProp, KLSNumpyFilled, KLSFinesNumpyFilled)\n",
    "\n",
    "#tester = analyseThisCFactorRaster(r\"C:\\DDrive\\MW\\v450projs\\Ram\\c-factors\\cf_201809_201811.tif\", \n",
    "#                                  cFactPattern, zonesRasterArray, zone_ds.GetGeoTransform(), \n",
    "#                                           zone_ds.RasterYSize, zone_ds.RasterXSize, NoData_value, defaultCfact, \n",
    "#                                           reqdValidProp, KLSNumpyFilled, KLSFinesNumpyFilled)\n",
    "\n",
    "#elapsed_time = time.time() - start_time\n",
    "#print(\"Finished One: \" + str(len(tester)) + \" taking this many seconds: \" + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing execute\n",
      "doing push\n",
      "starting pushing dict\n",
      "finished pushing dict\n",
      "Got this many rasters processed: 10 taking this many seconds: 169.70657467842102\n"
     ]
    }
   ],
   "source": [
    "#%load usle_cfactor_analysis\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "usleStats = {}\n",
    "\n",
    "mydict=dict(theZonesRasterArray=zonesRasterArray, datePattern=cFactPattern, zonesGT=zone_ds.GetGeoTransform(), ZonesRowCount=zone_ds.RasterYSize, \n",
    "            ZonesColCount=zone_ds.RasterXSize, ZoneND=NoData_value, \n",
    "            theDefaultC=defaultCfact, theReqValidProp=reqdValidProp, \n",
    "            theKLSNumpy=KLSNumpyFilled, theKLSFinesNumpy=KLSFinesNumpyFilled)\n",
    "\n",
    "\n",
    "#print(mydict)\n",
    "\n",
    "#num_cores = multiprocessing.cpu_count()\n",
    "#print(str(num_cores) + \" cores available\")\n",
    "\n",
    "#xxx = lambda cRas:usle_cfactor_analysis.analyseThisCFactorRaster(cRas,\n",
    "#                                                    zonesRasterArray, zone_ds.RasterYSize, \n",
    "#                                                    zone_ds.RasterXSize, NoData_value, \n",
    "#                                                    defaultCfact, reqdValidProp)\n",
    "\n",
    "#xxx = lambda cRas:analyseThisCFactorRaster(cRas, zonesRasterArray, zone_ds.GetGeoTransform(), \n",
    "#                                           zone_ds.RasterYSize, zone_ds.RasterXSize, \n",
    "#                                           NoData_value, defaultCfact, reqdValidProp)\n",
    "\n",
    "xxx = lambda theCFactorRas:analyseThisCFactorRaster(theCFactorRas, datePattern, theZonesRasterArray, zonesGT, \n",
    "                                           ZonesRowCount, ZonesColCount, ZoneND, theDefaultC, \n",
    "                                           theReqValidProp, theKLSNumpy, theKLSFinesNumpy)\n",
    "\n",
    "\n",
    "#xxx = lambda rasDate:usle_cfactor_analysis.analyseThisCFactorRaster(cRastersInfo.get(rasDate),\n",
    "#                                                    zonesRasterArray, zone_ds.RasterYSize, \n",
    "#                                                    zone_ds.RasterXSize, NoData_value, \n",
    "#                                                    defaultCfact, reqdValidProp)\n",
    "\n",
    "print(\"doing execute\")\n",
    "#rc[:].execute('import D:/PythonScriptingForGBR/reef_scripts/GBR/Examples/Modules/usle_cfactor_analysis')\n",
    "rc[:].execute(\"import numpy\")\n",
    "rc[:].execute(\"import gc\")\n",
    "rc[:].execute(\"import os\")\n",
    "rc[:].execute(\"import datetime\")\n",
    "rc[:].execute(\"from osgeo import gdal, gdalconst\")\n",
    "rc[:].execute(\"from osgeo import ogr\")\n",
    "\n",
    "print(\"doing push\")\n",
    "rc[:].push({\"analyseThisCFactorRaster\":analyseThisCFactorRaster})\n",
    "rc[:].push({\"centreCoordsForPixel\":centreCoordsForPixel})\n",
    "rc[:].push({\"pixelForCentreCoords\":pixelForCentreCoords})\n",
    "rc[:].push({\"getDateFromFileName\":getDateFromFileName})\n",
    "print(\"starting pushing dict\")\n",
    "rc[:].push(mydict)\n",
    "print(\"finished pushing dict\")\n",
    "results = rc[:].map_sync(xxx, cRastersAlone)\n",
    "#results = rc[:].map_sync(xxx, cRastersInfoKeysSorted)\n",
    "\n",
    "#results = rc[:].map_sync(lambda rasDate: usle_cfactor_analysis.analyseThisCFactorRaster(cRastersInfo.get(rasDate),\n",
    "#                                                    zonesRasterArray, zone_ds.RasterYSize, \n",
    "#                                                    zone_ds.RasterXSize, NoData_value, \n",
    "#                                                    defaultCfact, reqdValidProp), \n",
    "#                         cRastersInfoKeysSorted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#results = Parallel(n_jobs=num_cores)(delayed(analyseThisCFactorRaster)(cRastersInfo.get(rasDate), zonesRasterArray, zone_ds.RasterYSize, zone_ds.RasterXSize, NoData_value, defaultCfact, reqdValidProp) \n",
    "#                                     for rasDate in cRastersInfoKeysSorted)\n",
    "\n",
    "\n",
    "\n",
    "#for rasDate in cRastersInfoKeysSorted:\n",
    "    #Process the cFator raster\n",
    "    # processedStats will have the date as primary key and then each Unique FU ID and some relevant stats\n",
    "#    start_time = time.time()\n",
    "#    print(\"starting work on \" + str(cRastersInfo.get(rasDate)))\n",
    "    \n",
    "#    processedStats = analyseThisCFactorRaster(cRastersInfo.get(rasDate))\n",
    "    \n",
    "#    elapsed_time = time.time() - start_time\n",
    "    \n",
    "#    print(\"finished work on \" + str(cRastersInfo.get(rasDate)) + \" taking \" + str(elapsed_time) + \" seconds\")\n",
    "#    print(\"this big \" + str(len(processedStats)))\n",
    "    \n",
    "    \n",
    "#    #Add results to the stats\n",
    "#    usleStats[rasDate] = processedStats\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"Got this many rasters processed: \" + str(len(results)) + \" taking this many seconds: \" + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([datetime.date(2016, 3, 1)]) and 1\n",
      "dict_keys([datetime.date(2016, 6, 1)]) and 1\n",
      "dict_keys([datetime.date(2016, 9, 1)]) and 1\n",
      "dict_keys([datetime.date(2016, 12, 1)]) and 1\n",
      "dict_keys([datetime.date(2017, 3, 1)]) and 1\n",
      "dict_keys([datetime.date(2017, 6, 1)]) and 1\n",
      "dict_keys([datetime.date(2017, 9, 1)]) and 1\n",
      "dict_keys([datetime.date(2017, 12, 1)]) and 1\n",
      "dict_keys([datetime.date(2018, 3, 1)]) and 1\n",
      "dict_keys([datetime.date(2018, 6, 1)]) and 1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#type(results[0])\n",
    "#results[0].keys()\n",
    "convertedDict = {}\n",
    "availDates = []\n",
    "allConvertedToDict = {}\n",
    "for gg in results:\n",
    "    print(str(gg.keys()) + \" and \" + str(len(gg.keys())))\n",
    "    #Know that we only have one key, but need to loop anyway\n",
    "    for dateKey in gg.keys():\n",
    "        convertedDict[dateKey] = gg.values()\n",
    "        availDates.append(dateKey)\n",
    "        allConvertedToDict[dateKey] = {}\n",
    "        \n",
    "        for dataPairs in gg.values():\n",
    "            for fuKeys in dataPairs.keys():\n",
    "                #print(fuKeys)\n",
    "                allConvertedToDict[dateKey][fuKeys] = dataPairs[fuKeys]\n",
    "            #print(dataPairs.values())\n",
    "    #convertedDict[gg.keys()[0]] = gg.values()\n",
    "\n",
    "\n",
    "print(len(convertedDict))\n",
    "sortedAvailDates = sorted(availDates)\n",
    "#convertedDict\n",
    "#print(convertedDict[dateKey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([datetime.date(2016, 3, 1), datetime.date(2016, 6, 1), datetime.date(2016, 9, 1), datetime.date(2016, 12, 1), datetime.date(2017, 3, 1), datetime.date(2017, 6, 1), datetime.date(2017, 9, 1), datetime.date(2017, 12, 1), datetime.date(2018, 3, 1), datetime.date(2018, 6, 1)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convertedDict[datetime.date(1988, 3, 1)]\n",
    "convertedDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(allConvertedToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need FU-based KLSC & KLSCFines averages for tempral gap filling\n",
    "avgs = {}\n",
    "unZones = numpy.unique(zonesRasterArray)\n",
    "klscSumStr = \"KLSCSum\"\n",
    "klscfinesSumStr = \"KLSCFinesSum\"\n",
    "klscCountStr = \"KLSCCount\"\n",
    "klscfinesCountStr = \"KLSCFinesCount\"\n",
    "\n",
    "KLSCGlobalSum = 0\n",
    "KLSCGlobalCount = 0\n",
    "KLSCFinesGlobalSum = 0\n",
    "KLSCFinesGlobalCount = 0\n",
    "\n",
    "\n",
    "for zn in unZones:\n",
    "    avgs[zn] = {klscSumStr: float(0), klscCountStr: int(0), \n",
    "                klscfinesSumStr: float(0), klscfinesCountStr: int(0)}\n",
    "\n",
    "# now populate\n",
    "for dt in convertedDict.keys():\n",
    "    #print(type(convertedDict[dt]))\n",
    "    #print(convertedDict[dt])\n",
    "    for fuDict in convertedDict[dt]:\n",
    "        #print(type(fuDict))\n",
    "        for fuid in fuDict.keys():\n",
    "            #print(fuDict[fuid])\n",
    "\n",
    "            if not fuDict[fuid][KLSCvalStr] == -9999:\n",
    "                avgs[fuid][klscSumStr] += fuDict[fuid][KLSCvalStr]\n",
    "                avgs[fuid][klscCountStr] += 1\n",
    "                KLSCGlobalSum += fuDict[fuid][KLSCvalStr]\n",
    "                KLSCGlobalCount += 1\n",
    "            if not fuDict[fuid][KLSCFinesValStr] == -9999:\n",
    "                avgs[fuid][klscfinesSumStr] += fuDict[fuid][KLSCFinesValStr]\n",
    "                avgs[fuid][klscfinesCountStr] += 1\n",
    "                KLSCFinesGlobalSum += fuDict[fuid][KLSCFinesValStr]\n",
    "                KLSCFinesGlobalCount += 1\n",
    "\n",
    "#print(avgs[3])\n",
    "#print(avgs[36])\n",
    "#print(avgs[1397])\n",
    "KLSCGlobalAvg = 0\n",
    "KLSCFinesGlobalAvg = 0\n",
    "\n",
    "if KLSCGlobalCount > 0:\n",
    "    KLSCGlobalAvg = KLSCGlobalSum / KLSCGlobalCount\n",
    "\n",
    "if KLSCFinesGlobalCount > 0:\n",
    "    KLSCFinesGlobalAvg = KLSCFinesGlobalSum / KLSCFinesGlobalCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646\n",
      "{'KLSCSum': 0.005912427666897965, 'KLSCCount': 10, 'KLSCFinesSum': 0.0022027143038767173, 'KLSCFinesCount': 10}\n",
      "{'KLSCSum': 0.011885589769407692, 'KLSCCount': 10, 'KLSCFinesSum': 0.0033537220725977177, 'KLSCFinesCount': 10}\n",
      "{'KLSCSum': 0.0013430144216376818, 'KLSCCount': 10, 'KLSCFinesSum': 0.0006517087573206373, 'KLSCFinesCount': 10}\n"
     ]
    }
   ],
   "source": [
    "print(len(avgs))\n",
    "print(avgs[3])\n",
    "print(avgs[36])\n",
    "print(avgs[1397])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-01\n",
      "2018-06-01\n"
     ]
    }
   ],
   "source": [
    "# would now need to determine the required timespan for output timeseries\n",
    "# grabbing relevant values form either real or temporal average as needed\n",
    "\n",
    "reqdStart = datetime.date(2014, 1, 1)\n",
    "reqdLast = datetime.date(2017, 12, 31)\n",
    "\n",
    "earliestAvail = sortedAvailDates[0]\n",
    "latestAvail = sortedAvailDates[len(sortedAvailDates) - 1]\n",
    "\n",
    "print(earliestAvail)\n",
    "print(latestAvail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defs need padding at start\n"
     ]
    }
   ],
   "source": [
    "if reqdStart < earliestAvail:\n",
    "    print(\"Defs need padding at start\")\n",
    "else:\n",
    "    print(\"No start padding required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No end padding required\n"
     ]
    }
   ],
   "source": [
    "if reqdLast > latestAvail:\n",
    "    print(\"Defs need padding at end\")\n",
    "else:\n",
    "    print(\"No end padding required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-02-01\n",
      "2018-01-31\n",
      "2017\n",
      "12\n",
      "31\n",
      "2014-01-01\n",
      "2017-12-31\n",
      "[datetime.date(2014, 1, 1), datetime.date(2014, 2, 1), datetime.date(2014, 3, 1), datetime.date(2014, 4, 1), datetime.date(2014, 5, 1), datetime.date(2014, 6, 1), datetime.date(2014, 7, 1), datetime.date(2014, 8, 1), datetime.date(2014, 9, 1), datetime.date(2014, 10, 1), datetime.date(2014, 11, 1), datetime.date(2014, 12, 1), datetime.date(2015, 1, 1), datetime.date(2015, 2, 1), datetime.date(2015, 3, 1), datetime.date(2015, 4, 1), datetime.date(2015, 5, 1), datetime.date(2015, 6, 1), datetime.date(2015, 7, 1), datetime.date(2015, 8, 1), datetime.date(2015, 9, 1), datetime.date(2015, 10, 1), datetime.date(2015, 11, 1), datetime.date(2015, 12, 1), datetime.date(2016, 1, 1), datetime.date(2016, 2, 1), datetime.date(2016, 3, 1), datetime.date(2016, 4, 1), datetime.date(2016, 5, 1), datetime.date(2016, 6, 1), datetime.date(2016, 7, 1), datetime.date(2016, 8, 1), datetime.date(2016, 9, 1), datetime.date(2016, 10, 1), datetime.date(2016, 11, 1), datetime.date(2016, 12, 1), datetime.date(2017, 1, 1), datetime.date(2017, 2, 1), datetime.date(2017, 3, 1), datetime.date(2017, 4, 1), datetime.date(2017, 5, 1), datetime.date(2017, 6, 1), datetime.date(2017, 7, 1), datetime.date(2017, 8, 1), datetime.date(2017, 9, 1), datetime.date(2017, 10, 1), datetime.date(2017, 11, 1), datetime.date(2017, 12, 1)]\n"
     ]
    }
   ],
   "source": [
    "###I think I would then create a full list of required dates (varying with daily, monthly, seasonal, annual etc)\n",
    "###And then create a CSV writing loop based on indivudal FUs, where once processing a FU we begin stepping through\n",
    "### the full dates, looking for existence in convertedDict (taken from 'results')... although it has DATE as first key...\n",
    "###hmmm\n",
    "\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "\n",
    "bollocks = reqdStart + relativedelta(months=1)\n",
    "\n",
    "print(bollocks)\n",
    "\n",
    "bollocks2 = reqdLast + relativedelta(months=1)\n",
    "\n",
    "print(bollocks2)\n",
    "\n",
    "print(reqdLast.year)\n",
    "print(reqdLast.month)\n",
    "print(reqdLast.day)\n",
    "\n",
    "### Ok, manipulate start & end dates to 1st of month\n",
    "actualStartDate = datetime.date(reqdStart.year, reqdStart.month, 1)\n",
    "actualEndDate = datetime.date(reqdLast.year, reqdLast.month, calendar.monthrange(reqdLast.year, reqdLast.month)[1])\n",
    "print(actualStartDate)\n",
    "print(actualEndDate)\n",
    "\n",
    "theDates = []\n",
    "\n",
    "assessDate = actualStartDate\n",
    "\n",
    "while assessDate < actualEndDate:\n",
    "    theDates.append(assessDate)\n",
    "    assessDate = assessDate + relativedelta(months=1)\n",
    "\n",
    "\n",
    "print(theDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IntSCFU IntSCs        IntFUs\n",
      "0       1  SC #1  Conservation\n",
      "SC #1$Conservation\n"
     ]
    }
   ],
   "source": [
    "###Now try to get at the values... use FU = 1 to start....\n",
    "\n",
    "#print(fumapper)\n",
    "#\"'\" + str(theFUID) + \"'\"\n",
    "\n",
    "theFUID = 1\n",
    "theRow = fumapper.loc[fumapper[uniqueIDField] == str(theFUID)]\n",
    "print(theRow)\n",
    "theFuFileNameBit = str(theRow[catchmentField].values[0]) + \"$\" + str(theRow[FUField].values[0])\n",
    "\n",
    "print(theFuFileNameBit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listDateKeys = list(convertedDict.keys())\n",
    "##print(listTest)\n",
    "#print(listTest[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "lastRealDate = theDates[-1]\n",
    "#print(lastRealDate)\n",
    "\n",
    "for zn in unZones:\n",
    "    \n",
    "    if zn < 0:\n",
    "        continue\n",
    "    \n",
    "    theFUID = zn\n",
    "    #print(str(theFUID))\n",
    "    theRow = fumapper.loc[fumapper[uniqueIDField] == str(theFUID)]\n",
    "    #print(theRow)\n",
    "    theFuFileNameBit = str(theRow[catchmentField].values[0]) + \"$\" + str(theRow[FUField].values[0])\n",
    "\n",
    "    klscFile = open('C:/DDrive/MW/v450projs/Ram/tfiles/' + theFuFileNameBit + '$USLE_KLSC_Total.csv','w')\n",
    "    klscFinesFile = open('C:/DDrive/MW/v450projs/Ram/tfiles/' + theFuFileNameBit + 'USLE_KLSC_FinePerc.csv','w')\n",
    "\n",
    "    klscFile.write('Date,KLSC_Total' + '\\n')\n",
    "    klscFinesFile.write('Date,KLSC_Fines' + '\\n')\n",
    "    \n",
    "    \n",
    "    for thisDate in theDates:\n",
    "        theSep = '\\n'\n",
    "        ##Convert to seaonal date of relevance\n",
    "        seasonalDate = thisDate + relativedelta(months=monthToSeasonOffsets[thisDate.month])\n",
    "        if seasonalDate in allConvertedToDict.keys():\n",
    "            #print(\"In Here\")\n",
    "            #klscFile.write(str(thisDate) + theSep)\n",
    "            #print(allConvertedToDict[seasonalDate])\n",
    "            if theFUID in allConvertedToDict[seasonalDate]:\n",
    "                klscFile.write(str(thisDate) + ',' + str(allConvertedToDict[seasonalDate][theFUID][KLSCvalStr]) + theSep)\n",
    "                klscFinesFile.write(str(thisDate) + ',' + str(allConvertedToDict[seasonalDate][theFUID][KLSCFinesValStr]) + theSep)\n",
    "            else:\n",
    "                #Add in average\n",
    "                if theFUID in avgs:\n",
    "                    klscFile.write(str(thisDate) + ',' + str((avgs[theFUID][klscSumStr]/avgs[theFUID][klscCountStr])) + theSep)\n",
    "                    klscFinesFile.write(str(thisDate) + ',' + str((avgs[theFUID][klscfinesSumStr]/avgs[theFUID][klscfinesCountStr])) + theSep)\n",
    "                else:\n",
    "                    #Global avg???\n",
    "                    klscFile.write(str(thisDate) + ',' + str(KLSCGlobalAvg) + theSep)\n",
    "                    klscFinesFile.write(str(thisDate) + ',' + str(KLSCFinesGlobalAvg) + theSep)\n",
    "        else:\n",
    "            #Add in average\n",
    "            if theFUID in avgs:\n",
    "                klscFile.write(str(thisDate) + ',' + str((avgs[theFUID][klscSumStr]/avgs[theFUID][klscCountStr])) + theSep)\n",
    "                klscFinesFile.write(str(thisDate) + ',' + str((avgs[theFUID][klscfinesSumStr]/avgs[theFUID][klscfinesCountStr])) + theSep)\n",
    "            else:\n",
    "                #Global avg???\n",
    "                klscFile.write(str(thisDate) + ',' + str(KLSCGlobalAvg) + theSep)\n",
    "                klscFinesFile.write(str(thisDate) + ',' + str(KLSCFinesGlobalAvg) + theSep)\n",
    "\n",
    "    klscFile.close()\n",
    "    klscFinesFile.close()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
